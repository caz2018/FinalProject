---
title: "WeatherCurrencyAndSales"
author: "Caroline Silvestre"
date: "30/09/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#A program for analysing the relationship between weather patterns, currency values and sales volumes, given a series of daily sale values of a particular product.

```{r uploading the sales dataset}

#clean up env
rm(list = ls())

library(readr)
library(stats)
library(dplyr)
library(reshape2)
library(lubridate)
library(ggplot2)
library(forecast)
library(BBmisc)
library(zoo)

#reads data from csv and loading sales data with dates and formatting the date field as date in R
#Note: file must be a csv and the date order must be oldest to newest in the file.
SalesData <-  as.data.frame(read_csv("SalesReportDaily.csv"))
```

```{r summary, echo=FALSE}
print(summary(SalesData))
```

```{r manipulating variables before analysis}

require(lubridate)
date <- ymd(SalesData$Date)
SalesData$Date <- date
SalesData$DoW <- as.factor(weekdays.Date(date))
SalesData$WeekNum <- recode(SalesData$DoW,'Monday'=1,'Tuesday'=2, 'Wednesday'=3,'Thursday'=4,
                                        'Friday'=5,'Saturday'=6,'Sunday'=7)
### add the number of the month
require("lubridate")
SalesData$MonthNum <- month(SalesData$Date)
SalesData$MonthFactor <- as.factor(recode(SalesData$MonthNum, '1'="Jan", '2'= "Feb", '3'= "Mar", '4'= "Apr", '5'= "May", '6'= "Jun", '7'="Jul",'8' = "Aug", '9'= "Sep", '10' = "Oct", '11'= "Nov", '12' = "Dec"))

#setting the seasonality variables
weekly <- 7
monthly <- 30.5
yearly <- 364.25

#adding a normalized version of daily sales values to the dataframe, using a 1 to 10 range to avoid zero values
SalesData$ValNorm <- normalize(SalesData$Total_Daily_Value , method="range", range=c(1,10))

#need to transform sales data into ts (time series) objects and then into a msts (multi-seasonal time-series). The default is daily.
#function to transform a vector into a time series object. Need the day of the year as a number from 1 to 365/366.
#freq is the data interval, for daily data use yearly.

ts.transform <- function(univariateseries, YYYY,DDD, freq){
  x = ts(univariateseries, start = c(YYYY,DDD) , frequency = freq)
  return(x)
}

tsVal <- ts.transform(SalesData$Total_Daily_Value, 2014,001, yearly)
```
```{r timeseries, echo=FALSE}
autoplot(tsVal) + ylab("Daily Value")+ xlab("Year")
```

```{r time series}
#time series for all variables were created, but not all were used in the current analysis.
tsItems <- ts.transform(SalesData$Daily_Items_Sold, 2014,001, yearly)
#autoplot.zoo(tsItems) + ylab("Daily Items") + xlab("Year")

tsATV <- ts.transform(SalesData$Avg_Trans_Value_GBP, 2014,001, yearly)
#autoplot.zoo(tsATV) + ylab("Daily Items") + xlab("Year")

tsValNorm <- ts.transform(SalesData$ValNorm,2014,001, yearly)
#autoplot(tsValNorm) + ylab("Sales Volume Normalised") + xlab("Year")

#function for transforming a time series object into a msts, which is a multiple-series time series
require(forecast)
msts.transform <- function(tsobj, YYYY,DDD,s1=NULL,s2=NULL,s3=NULL){
  a = msts(tsobj, start = c(2014,001), seasonal.periods=c(s1,s2,s3))
  return(a)
}

#function to decompose both single-seasonal and multi-seasonal time-series
decomp.msts <- function(mstsobj){
  xy1 = mstl(mstsobj, iterate = 3)
  return(xy1)
}

#By default the Full Daily Sales Value will be used for ease of visualisation, with yearly and weekly pattern, but only normlised plots will be generated for better visualisation
mstsVal <- msts.transform(tsVal, 2014,001, weekly, yearly)
decVals <- decomp.msts(mstsVal)
```

Now the Daily Sales Values will be decomposed using Seasonal Decomposition of Time Series by Loess, which finds the seasonal components by smoothing each seasonal subseries (for example, January values, Monday Values, etc). Once those components are found, they are subtracted from the data and the remainder is smoothed to find the trend component.  
```{r}
#generating a decomposition of the normalised Daily sales values for use in the analysis in conjunction with other variables.
salesNormmsts_mwy <- msts(tsValNorm, seasonal.periods=c(7,30.5,365.25))
xnorm_wmy <- mstl(salesNormmsts_mwy, lambda = "auto", iterate = 3)

#generating a dataset with decomposed values to use with GAM
dat_decomp <- as.data.frame(xnorm_wmy)
dat_decomp$Date <- date

#mergin the decomposition to the original data frame
SalesData<- merge(SalesData, dat_decomp, by.x = "Date", by.y = "Date", all.x = TRUE)
```

Here the Daily Sales time-series was decomposed into five elements:
- Trend
- Weekly seasonality (7 day period)
- Monthly seasonality (30.5 day period)
- Yearly seasonality (365.25 day period)
```{r, echo=FALSE}
autoplot(xnorm_wmy)+ylab("Daily Value") + xlab("Year")
```
It is possible to see that the strongest component is the Yearly one, showing a large peak in sales in the last quarter of each year.
There is also a remarkable upward trend in the data.


#Notes on forecasting based on sales data and seasonality only

When x is a time-series object, K should be a vector of integers specifying the
number of sine and cosine (Fourier) terms to be used with each of the seasonal periods (if more than one) for generating a forecast.
The minimum number is 1, and the maximum number is half as many periods of season are in the data. So for yearly periods, the maximum K is 365.25/ 2 (only integer values are allowed).
Therefore, the best way to find a suitable number of Fourier terms, it is advisable to do it by computing the lowest AICc.
However, due to the large amount of data, the code to make this calculation takes several minutes to run (over 30 minutes). The code below had been added for reference, but it will not be used.
This program will not be running a forecast using ARIMA with fourier terms for two reasons: First is that the calculation for large datasets with more than one seasonal element takes too long, and second is that this model does not allow for the introduction of extra variables.
```{r}
#salesmsts = msts(timeseriesValue, start = c(2014,001), seasonal.periods=c(365.25))
#bestfit <- list(aicc=Inf)
#for(K in seq(25)) {
 # fit <- auto.arima(salesmsts, xreg=fourier(salesmsts, K=K),
  #                  seasonal=FALSE)
  #if(fit[["aicc"]] < bestfit[["aicc"]]) {
   # bestfit <- fit
    #bestK <- K
  #}
#}
#fc <- forecast(bestfit,
 #              xreg=fourier(salesmsts, K=bestK, h=6*365.25))

#autoplot(fc)

#fit[["aicc"]]

#arima fcast with week and year seasonalities, for future comparison with GAM models
#the value of K = 19 was found by running a script for a few hours, based on the lowest AIcc
#The code below retunrs forecasting using a dynamic regression for 90 days ahead
#the code below takes several minutes to run

#for predicting sales using the arima decomposition only for comparison to the gam
#fitwy <- auto.arima(mstsVal, seasonal=FALSE,
                      #xreg=fourier(mstsVal, K=c(3,19)))
#fitwy %>% forecast(xreg=fourier(salesmsts, K=c(3,19), h=1*90)) %>% autoplot(include=6*336)'
```

Joining historical weather observations values and currency exchange vales by date.

```{r}

WeatherData <- read_csv("WeatherDaily.csv")
weatherdates <- as.Date(WeatherData$DATE, "%d/%m/%Y")
#View(weatherdates)
WeatherData$DATE <- weatherdates
#remove NA's
WeatherData$PRCP[is.na(WeatherData$PRCP)] <- 0

#uploading currency data
CurrencyData <- CurrencyExchangeDaily <- read_csv("CurrencyExchangeDaily.csv")
currencydates <- mdy(CurrencyData$Date)
CurrencyData$Date <- currencydates
#trading stops on weekends, so weekends will carry the latest value after the merge of datasets

#removing data we don't need
CurrencyData$Open <- NULL
CurrencyData$High <- NULL
CurrencyData$Low <- NULL
CurrencyData$Change <- NULL


#merging weather and sales dataframes
newdf <- merge(SalesData, WeatherData, by.x = "Date", by.y = "DATE", all.x = TRUE)
#adding currency data
newdf <- merge(newdf, CurrencyData, by.x = "Date", by.y = "Date", all.x = TRUE)
#replacing NA's in currency data with the last value 
newdf$Price <- na.locf(newdf$Price)

#transforming precipitation into factors - 'rain' and 'dry'
newdf$PrecFact <- newdf$PRCP[newdf$PRCP > 0] <- 1
newdf$PrecFact <- as.factor(newdf$PRCP)

#running a seasonal decomposition on weather as it is a highly seasonal feature
tavg.ts <- ts(newdf$TAVG)
tavg.msts <- msts.transform(tavg.ts,2014,001,yearly)
tavg.decomp <- decomp.msts(tavg.msts)


summary(newdf)
```
#Seasonaly decomposing the daily weather data
```{r weather seasonal decomposition, echo= FALSE}
#for plotting the weather decomposition
autoplot(tavg.decomp) + ylab("Daily Value") + xlab("Year")
```
Because weather is seasonal as well as sales, decomposing the seasonality from the weather and using the remainder element of decomposition seemed like the best way of retrieving 'unusually' cold or hot days for use in the analysis, instead of actual values. This is because the peak of sales is on the runup to the Chrstmas period, which coincides with the colder months in the nothern hemisphere. If this same analysis were run on the southern hemispehere, there is a change that the results would be completely different.

```{r}
Temperature  <- as.data.frame(tavg.decomp)
Temperature$Date <- newdf$Date
#merge datasets again
newdf <- merge(newdf, Temperature, by.x = "Date", by.y = "Date", all.x = TRUE )

#change days of week and month as factors
newdf$DoW <- as.factor(newdf$DoW)
newdf$MonthFactor <- as.factor(newdf$MonthFactor)
```
The merging of all data that has been extracted, manipulated or kept as original, has resulted in a large dataset. From those resulting variables, different models will be tested for accuracy of forecast.
```{r}
summary(newdf)
```
#Determining variable correlation using Generalized additive models with integrated smoothness estimation.

```{r}
library(mgcv)
#reading dataframe as data.table
DT <- as.data.frame.table(newdf)
summary(DT)

#Setting variables

n_value <- unique(DT[, "Freq.Total_Daily_Value"])
n_date <- unique(DT[, "Freq.Date"])
n_weekdays <- unique(DT[, "Freq.WeekNum"])
n_dow <- unique(DT[, "Freq.DoW"])
n_monthFact <- unique(DT[, "Freq.MonthFactor" ])
n_monthn <- unique(DT[, "Freq.MonthNum" ])
n_temp <- unique(DT[, "Freq.TAVG"])
n_rain <- unique(DT[, "Freq.PRCP"])
n_price <- unique(DT[, "Freq.Price"])
n_temp_variation <- unique(DT[, "Freq.Remainder.y"])
week_period <- 7
year_period <- 365.25
#back-up
data_r <- DT
```
The plot below highlights the relationship between the price of the British Pound in US Dollars and Daily sales values. It is possible to see an inversed correlation, where the cheper the GBP is in comparison to the USD, the higher the sales value.

```{r, echo=FALSE}
#Currency close-up - needs ordering

ggplot(data_r, aes( data_r$Freq.Price , data_r$Freq.Total_Daily_Value)) +
        geom_point()+
        theme(panel.border = element_blank(),
        panel.background = element_blank(),
        panel.grid.minor = element_line(colour = "grey90"),
        panel.grid.major = element_line(colour = "grey90"),
        panel.grid.major.x = element_line(colour = "grey90"),
        axis.text = element_text(size = 8),
        axis.title = element_text(size = 8, face = "bold")) +
  labs(x = "Currency", y = "Avg Daily Sales GBP")

```

The next plot shows the relationship between Sales and how 'unusual' the temperature for a particular day was. This is done by using the remaineder element from the decomposition that was previously done on the daily weather values.
The relationship does not seem strong, however it is possible to see that the highest sales numbers tend to lean towards to coldest days.
```{r, echo=FALSE}
#Weather close-up - needs ordering

ggplot(data_r, aes( data_r$Freq.Remainder.y, data_r$Freq.Total_Daily_Value)) +
        geom_point()+
        theme(panel.border = element_blank(),
        panel.background = element_blank(),
        panel.grid.minor = element_line(colour = "grey90"),
        panel.grid.major = element_line(colour = "grey90"),
        panel.grid.major.x = element_line(colour = "grey90"),
        axis.text = element_text(size = 8),
        axis.title = element_text(size = 8, face = "bold")) +
  labs(x = "Daily Temperatue Variance (unusually cold to unusually hot", y = "Avg Daily Sales GBP")

```

```{r}
#Precipitation close-up - needs ordering
ggplot(data_r, aes( data_r$Freq.PrecFact, data_r$Freq.Total_Daily_Value)) +
geom_boxplot()

```

```{r}

N <- nrow(data_r) # number of observations

window <- N / 365.25 # number of periods in the train set
require(data.table)
require(mgcv)
#generating a matrix of values representing each variable to be included in the Generalized additive models (GAMs)
matrix_gam <- data.table(Sales = data_r[ ,"Freq.Total_Daily_Value" ],
                         Monthly = data_r[,"Freq.MonthNum"],
                        Weekly = data_r[,"Freq.WeekNum" ],
                        Temp = data_r[,"Freq.Remainder.y"],
                        price = data_r[,"Freq.Price" ],
                        trend = data_r[,"Freq.Trend.x"]
                       )



#individual gams on Day Of The Week and Month of The Year only, for visualisation of its effects
gam_0 <- gam(Sales ~ s(Weekly, bs = "ps", k = week_period) +
               s(Monthly, bs = "cr", k = 12) + 
               s(price)+
               s(Temp),
             data = matrix_gam,
             family = gaussian) 

print(summary(gam_0)$s.table)
```
```{r}
summary(gam_0)
```

Running the models with each variable individually (excluding trend), shows effects that can be interpreted on their own:
```{r, echo=FALSE}
layout(matrix(1:4, nrow = 1))
plot(gam_0, shade = TRUE, main = "Single variable models")

```
- Weekly: From 1 (Monday) to 7 (Sunday): It would seem Saturdays are the lowest expected trading days, while Sunday the highest, although the p-value for this variable was too high (2.578628e-291), so this variable is not significant. 
- Monthly: From 1 (Januray) to 12 (December): The highest volumes could be expected in November, the lowers in the summer months.
- price: Price of GBP in USD - The highest volumes were seen when the price of GBP was lowest
- Temp (Temperature variation): This is a range of how 'off' the moving average the temperature was for a given day. It seems that lower volumes were observed for very unusually cold days, slightly up for slightly colder days, slightly down for slightly hotter days, but the spike in unusually warm days seems a difficult to explain, as indeed the p-value seems too high.

```{r}

dispsumm <- function(g, gdesc){
  print(summary(g)$s.table)
  print(summary(g)$sp.criterion)
  cat(gdesc, "\n", "GAM R-sq:", summary(g)$r.sq,"\n", "aic: ", g$aic, "\n", g$GCV, "\n")
  
  }


gam_1_desc <- "GAM.1 - Week and month elements only"
gam_1 <- gam(Sales ~ s(Weekly, Monthly),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_1, gam_1_desc)


gam_2_desc <-"GAM.2 - Week, month and trend elements"
gam_2 <- gam(Sales ~ s(Weekly, Monthly, trend),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_2, gam_2_desc)

gam_3_desc <-"GAM.3 Week, month, and trend + temperature"
gam_3 <- gam(Sales ~ s(Weekly, Monthly, Temp, trend),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_3, gam_3_desc)


gam_4_desc <- "GAM.4  Week, month, and trend + currency price"
gam_4 <- gam(Sales ~ s(Weekly, Monthly, price, trend),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_4, gam_4_desc)

gam_5_desc <- "GAM.5  Week, month, and trend + temperature + currency price"
gam_5 <- gam(Sales ~ s(Weekly, Monthly, Temp, price, trend),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_5, gam_5_desc)

gam_6_desc <-"GAM.6  Week, month + currency price only (trend removed)"
gam_6 <- gam(Sales ~ s(Weekly, Monthly, price),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_6, gam_6_desc)

gam_7_desc <-"GAM.7  Week, month + temperature elements only (trend removed"
gam_7 <- gam(Sales ~ s(Weekly, Monthly, Temp),
             data = matrix_gam,
             family = gaussian)
dispsumm(gam_7, gam_7_desc)
```

```{r finding the best model by lowest aic}
bestfit <- list(aic=Inf)
bestgam <- "none"
gams <- c(gam_1$aic,gam_2$aic,gam_3$aic,gam_4$aic,gam_5$aic,gam_6$aic,gam_7$aic)
x=0
for (i in gams){
  x <- x+1
  if(i < bestfit$aic) {
    bestfit$aic <- i
    bestgam <- x
    } 
}
cat("The GAM with lowest aic is the number",bestgam)
```



```{r, echo=FALSE}

#Plotting fitted values of the bestfitted GAM
df2 <- data.table(value = gam_5$fitted.values, data_time = data_r[ , "Freq.Date"] )
df1 <- data.table(value = data_r$Freq.Total_Daily_Value, data_time = data_r[ , "Freq.Date"] )
df2$type <- "Real"
df1$type <- "Fitted"

datas <- rbind(df1,df2)
  
datas[, type := c(rep("Real", nrow(data_r)), rep("Fitted", nrow(data_r)))]

ggplot(data = datas, aes(data_time, value, group = type, colour = type)) +
  geom_line(size = 0.6) +
  theme_bw() +
  labs(x = "Time", y = "Sales",
       title = "Fit from GAM.5 - Modeled on Week, month, and trend + temperature + currency price ")
```


```{r, echo=FALSE}

#Plotting fitted values
dfs1 <- data.table(value = gam_6$fitted.values, data_time = data_r[ , "Freq.Date"] )
dfs2 <- data.table(value = data_r$Freq.Total_Daily_Value, data_time = data_r[ , "Freq.Date"] )
dfs1$type <- "Fitted"
dfs2$type <- "Real"

dataseason <- rbind(dfs1,dfs2) 

dataseason[, type := c(rep("Fitted", nrow(data_r)), rep("Real", nrow(data_r)))]

ggplot(data = dataseason, aes(data_time, value, group = type, colour = type)) +
  geom_line(size = 0.6) +
  theme_bw() +
  labs(x = "Time", y = "Sales",
       title = "Fit from GAM n.6 - Week, month + currency price only (trend removed)")
```
Plotting predicted values based on currency price and seasonality(Month and Week) return a


#Plotting sales values and fx

```{r, echo= FALSE} 
dffx1 <- data.table(value = data_r$Freq.Price*100000, data_time = data_r[ , "Freq.Date"] )
dffx2 <- data.table(value = data_r$Freq.Total_Daily_Value, data_time = data_r[ , "Freq.Date"] )
dffx1$type <- "GBP price in USD"
dffx2$type <- "Sales"

dataseason <- rbind(dffx1,dffx2) 

dataseason[, type := c(rep("GBP price in USD", nrow(data_r)), rep("Sales", nrow(data_r)))]

ggplot(data = dataseason, aes(data_time, value, group = type, colour = type)) +
  geom_line(size = 0.8) +
  theme_bw() +
  labs(x = "Time", y = "Sales",
       title = "Sales vs USD to GBP")
```
#Plotting sales values and weather

```{r, echo= FALSE} 
dffx1 <- data.table(value = data_r$Freq.Remainder.y, data_time = data_r[ , "Freq.Date"] )
dffx2 <- data.table(value = data_r$Freq.ValNorm , data_time = data_r[ , "Freq.Date"] )
dffx1$type <- "temperature"
dffx2$type <- "Sales"

dataseason <- rbind(dffx1,dffx2) 

dataseason[, type := c(rep("weather", nrow(data_r)), rep("Sales", nrow(data_r)))]

ggplot(data = dataseason, aes(data_time, value, group = type, colour = type)) +
  geom_line(size = 0.3) +
  theme_bw() +
  labs(x = "Time", y = "Sales",
       title = "Fit from weather noise to normalised values")
```
Future developments to this program include:
- Support for multiple currencies
- User-interface Support for visualization of the predicted value of sale when the user provides a currency value and a date





